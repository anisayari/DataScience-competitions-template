{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Notebook - DataScience Competition Baseline\n",
    "\n",
    "### Created by Anis Ayari : https://github.com/anisayari on May 2019\n",
    "\n",
    "Please consider to report any enhancements/bug/modification/use to : aayari@deloitte.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/datascience-env/lib/python3.6/site-packages/matplotlib/__init__.py:1003: UserWarning: Duplicate key in file \"/Users/anisayari/.matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/anaconda3/envs/datascience-env/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#DS & Math\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "#Vizu libraries\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#sklearn libraries\n",
    "from sklearn.decomposition import TruncatedSVD,NMF\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer, OneHotEncoder\n",
    "from sklearn.model_selection import KFold,cross_val_score,cross_val_predict, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier, BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Other ML libraries\n",
    "import featuretools as ft\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "from stop_words import get_stop_words\n",
    "stop_words_fr = get_stop_words('fr')\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "\n",
    "#Others\n",
    "import cv2\n",
    "import warnings\n",
    "import csv \n",
    "import os \n",
    "import time \n",
    "import urllib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FEATURE ENGINEERING COMMON FUNCTIONS\n",
    "\"\"\"\n",
    "#@TODO : 'Need to check with auto FE libraries\n",
    "\"\"\"\n",
    "MATHEMATICS FEATURES\n",
    "\"\"\"\n",
    "def create_mathematics_features(df, column_to_count, column_to_groupby):\n",
    "    df_tmp = df.groupby(column_to_groupby)[column_to_count].agg(['count','mean', 'std', 'max', 'min'])\n",
    "    df_tmp.columns =['count_' + column_to_count, 'mean_' + column_to_count, 'std_' + column_to_count,'max_' + column_to_count, 'min_' +column_to_count,]\n",
    "    df = df.merge(df_tmp, on=column_to_groupby, how='left')\n",
    "    return df \n",
    "\n",
    "\"\"\"\n",
    "NUMERICAL FEATURES\n",
    "\"\"\"\n",
    "def get_len_columns(df, len_columns):\n",
    "    for col_ in len_columns:\n",
    "        df[\"len_\" + col_] = df[col_].str.len()\n",
    "    return df\n",
    "\n",
    "def transform_to_log(df,columns_to_log):\n",
    "    for col_ in columns_to_log:\n",
    "        df['log_' + col_] = (1+df[col_]).apply(np.log)\n",
    "    return df\n",
    "\n",
    "def count_product_per_store(df, column_to_groupby, column_to_count):\n",
    "    tmp = df.groupby(column_to_groupby).count()[column_to_count].reset_index()\n",
    "    tmp.columns = [column_to_groupby] + [\"number_\" + column_to_count + '_' + column_to_groupby]\n",
    "    df = df.merge(tmp, on=column_to_groupby, how='left')\n",
    "    return df\n",
    "\n",
    "def count_item_column(df, column_to_count, column_groupby):\n",
    "    rescuer_count = df.groupby([column_to_count])[column_groupby].count().reset_index()\n",
    "    rescuer_count.rename(columns={rescuer_count.columns[0]: column_to_count}, inplace=True)\n",
    "    rescuer_count.columns = [column_to_count, column_to_count+'_COUNT']\n",
    "    df = df.merge(rescuer_count, how='left', on=column_to_count)\n",
    "    return df\n",
    "\n",
    "def label_encoding(df,columns_to_encode):\n",
    "    labelencoder = LabelEncoder()\n",
    "    categ_cols = columns_to_encode\n",
    "    for columns_ in categ_cols:\n",
    "        df[columns_+'_ENCODED'] = labelencoder.fit_transform(df[columns_].values.astype(str))\n",
    "    return df\n",
    "\n",
    "def binarie_fill(df,column):\n",
    "    df[column] = df[column].fillna(0)\n",
    "    if True in df[column].tolist():\n",
    "        df[column]= np.where(df[column]==True,1,0)\n",
    "    else:\n",
    "        df[column]= np.where(df[column]==0,0,1)\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "TEXT\n",
    "\"\"\"\n",
    "\n",
    "def apply_tfidf_vectorizer(df, column):\n",
    "    df[column] = df[column].fillna(\"missing\")\n",
    "    df[column] = df[column].astype(str)\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3), stop_words = stop_words_fr, lowercase=True, \n",
    "                                     max_features=50, binary=True, norm=None,use_idf=False)\n",
    "    tfidf = vectorizer.fit_transform(df[column])\n",
    "    tfidf_cols = vectorizer.get_feature_names()\n",
    "    tmp = pd.DataFrame(data=tfidf.toarray(), columns=['tfidf_' + column + '_' + i for i in tfidf_cols])\n",
    "    df = pd.concat([df, tmp], axis=1,sort=False)\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "IMAGE\n",
    "\"\"\"\n",
    "#@TODO : 'To fill'\n",
    "\n",
    "\"\"\"\n",
    "SONG\n",
    "\"\"\"\n",
    "#@TODO : 'To fill'\n",
    "\n",
    "\n",
    "def tfidf_nmf_svd(df,text_columns):\n",
    "    for col_ in tqdm(text_columns):\n",
    "        text = df[col_].values.tolist()\n",
    "        cvec = CountVectorizer(min_df=2, ngram_range=(1, 3), max_features=1000,\n",
    "                               strip_accents='unicode',\n",
    "                               lowercase=True, analyzer='word', token_pattern=r'\\w+',\n",
    "                               stop_words=stop_words_fr)\n",
    "        text = [str(element) for element in text]\n",
    "        cvec.fit(text)\n",
    "        X = cvec.transform(text)\n",
    "        df['cvec_sum'] = X.sum(axis=1)\n",
    "        df['cvec_mean'] = X.mean(axis=1)\n",
    "        df['cvec_len'] = (X != 0).sum(axis=1)\n",
    "        tfv = TfidfVectorizer(min_df=2, max_features=200,\n",
    "                              strip_accents='unicode', analyzer='word',\n",
    "                              ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                              stop_words=stop_words_fr)\n",
    "\n",
    "        # Fit TFIDF\n",
    "        X = tfv.fit_transform(text)\n",
    "        df['tfidf_sum'] = X.sum(axis=1)\n",
    "        df['tfidf_mean'] = X.mean(axis=1)\n",
    "        df['tfidf_len'] = (X != 0).sum(axis=1)\n",
    "        \n",
    "        \"\"\"\n",
    "        n_components = 20\n",
    "\n",
    "        print('[INFO] Start NMF')\n",
    "\n",
    "        nmf_ = NMF(n_components=n_components)\n",
    "        X_nmf = nmf_.fit_transform(X)\n",
    "        X_nmf = pd.DataFrame(X_nmf, columns=['{}_nmf_{}'.format(col_, i) for i in range(n_components)])\n",
    "        X_nmf['id'] = df.id.values.tolist()\n",
    "        df = pd.concat([df.set_index('id'), X_nmf.set_index('id')], sort=False, axis=1).reset_index()\n",
    "        df.rename(columns={df.columns[0]: 'id'}, inplace=True)\n",
    "\n",
    "        print('[INFO] Start SVD')\n",
    "        svd = TruncatedSVD(n_components=n_components)\n",
    "        svd.fit(X)\n",
    "        print('fit done')\n",
    "        X_svd = svd.transform(X)\n",
    "        X_svd = pd.DataFrame(X_svd, columns=['{}_svd_{}'.format(col_, i) for i in range(n_components)])\n",
    "        X_svd['id'] = df.id.values.tolist()\n",
    "        df = pd.concat([df.set_index('id'), X_svd.set_index('id')], sort=False, axis=1).reset_index()\n",
    "        df.rename(columns={df.columns[0]: 'id'}, inplace=True)\n",
    "        df.drop(col_, axis=1, inplace=True)\n",
    "        \"\"\"\n",
    "        \n",
    "    return df\n",
    "\n",
    "def auto_features(df):\n",
    "    print('[INFO] Auto Features Processing')\n",
    "    \n",
    "    es = ft.EntitySet(id = 'emmaus')\n",
    "    #es = es.entity_from_dataframe(entity_id = 'data',dataframe = train_test.reset_index(drop=True),make_index = True,index='id')\n",
    "    es = es.entity_from_dataframe(entity_id='data', index='id', dataframe = df)\n",
    "\n",
    "    for groupby in ['brand','category','store_name','product_name','material']:\n",
    "        es = es.normalize_entity(base_entity_id='data', new_entity_id=groupby, index=groupby)\n",
    "    \n",
    "    features, feature_names = ft.dfs(entityset = es, target_entity = 'data', max_depth = 2, verbose=2, n_jobs=5)\n",
    "\n",
    "    df = df.set_index('id').append([features], sort=False)\n",
    "    return df,feature_names\n",
    "\n",
    "def drop_higlhy_correlated_features(df):\n",
    "    # Threshold for removing correlated variables\n",
    "    threshold = 0.95\n",
    "\n",
    "    # Absolute value correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    upper.head(50)\n",
    "\n",
    "    # Select columns with correlations above threshold\n",
    "    collinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "    print('There are %d features to remove.' % (len(collinear_features)))\n",
    "\n",
    "    features_filtered = df.drop(columns = collinear_features)\n",
    "\n",
    "    print('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])\n",
    "    features_positive = features_filtered.loc[:, features_filtered.all()]\n",
    "    return features_positive,features_filtered\n",
    "\n",
    "\n",
    "def features_engineering(df):\n",
    "    \"\"\"\n",
    "    DROP NOT RELEVANT COLUMN \n",
    "    \"\"\"\n",
    "    print('[INFO] Dropping Columns...')\n",
    "    columns_to_drop = [\"image_url\", \"sub_category_3\", \"sub_category_4\"]  #'To fill'\n",
    "    df.drop(columns_to_drop, axis = 1, inplace = True)    \n",
    "    \n",
    "    text_columns = df.select_dtypes(include='object').columns.tolist()\n",
    "    df[text_columns] = df[text_columns].fillna('missing')\n",
    "    \n",
    "    df,features_filtered = auto_features(df)\n",
    "    df = df.reset_index()\n",
    "\n",
    "    \"\"\"\n",
    "    TEXT FEATURES\n",
    "    \"\"\"\n",
    "    print('[INFO] Text Features processing')\n",
    "    \n",
    "    df = get_len_columns(df, len_columns=['product_description'])\n",
    "    \n",
    "    df = label_encoding(df, columns_to_encode=['color','age','product_size',\"brand\",\"shoe_size\"] )\n",
    "        \n",
    "    #count_column = [\"brand\", \"author\", \"editor\"]  #'To fill'\n",
    "    #for col_ in count_column:\n",
    "        #df = count_item_column(df, col_, 'id')\n",
    "    \n",
    "    column_to_vectorize = [\"sub_category_1\", \"sub_category_2\",'store_name','product_description',\n",
    "                    'material', 'editor', 'product_name',\"author\"]  #'To fill'\n",
    "    \n",
    "    #for column_ in column_to_vectorize:\n",
    "        #if column_ in df.columns :\n",
    "            #df=apply_tfidf_vectorizer(df,column_)\n",
    "            #df.drop(column_, inplace=True, axis=1)\n",
    "    df=tfidf_nmf_svd(df,text_columns=column_to_vectorize)\n",
    "    \n",
    "    binary_column = ['warranty','wifi','vintage']  #'To fill'\n",
    "    for col_ in tqdm(binary_column):\n",
    "        df = binarie_fill(df,col_)\n",
    "    \n",
    "    columns_to_dummies = ['category']  # 'To fill'\n",
    "    for col_ in tqdm(columns_to_dummies):\n",
    "        df = pd.concat([df.drop(col_, axis=1), pd.get_dummies(df[col_],prefix=col_)], axis=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    NUMERICAL FEATURES\n",
    "    \"\"\"\n",
    "    column_to_count = 'price'    #'To fill'\n",
    "    column_to_groupby = 'store_name'    #'To fill'\n",
    "    #df = create_mathematics_features(df, column_to_count, column_to_groupby)\n",
    "    \n",
    "    \n",
    "    columns_to_log = [\"price\", \"len_product_description\"]  #'To fill'\n",
    "    transform_to_log(df,columns_to_log)\n",
    "\n",
    "    #to_drop = [\"price\",\"id\",'image_width','image_height','color','age','product_size',\"brand\",\"shoe_size\",\"len_product_description\", \"condition\", \"year\", \"product_width\",\"product_length\", \"product_height\"]  #'To fill'\n",
    "    #df.drop(to_drop,inplace=True, axis=1)\n",
    "    #df,features_filtered=drop_higlhy_correlated_features(df)\n",
    "    train_test.drop('id',axis=1)\n",
    "    \n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df, id_column, column_path):\n",
    "    print('here______________________________')\n",
    "    from tensorflow.keras.applications.densenet import preprocess_input, DenseNet121\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n",
    "    import tensorflow.keras.backend as K\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.client import device_lib\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    sess = tf.Session()\n",
    "\n",
    "    def get_available_gpus():\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "    print(get_available_gpus())\n",
    "\n",
    "    def load_image(img_size, path='', url=''):\n",
    "        def resize_to_square(im, img_size):\n",
    "            old_size = im.shape[:2]  # old_size is in (height, width) format\n",
    "            ratio = float(img_size) / max(old_size)\n",
    "            new_size = tuple([int(x * ratio) for x in old_size])\n",
    "            # new_size should be in (width, height) format\n",
    "            im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "            delta_w = img_size - new_size[1]\n",
    "            delta_h = img_size - new_size[0]\n",
    "            top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "            left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "            color = [0, 0, 0]\n",
    "            new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "            return new_im\n",
    "        if url =='':\n",
    "            image = cv2.imread(path)\n",
    "            \n",
    "        elif path=='': \n",
    "                # download the image, convert it to a NumPy array, and then read\n",
    "            # it into OpenCV format\n",
    "            resp = urllib.request.urlopen(url)\n",
    "            image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "            image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "        new_image = resize_to_square(image, img_size)\n",
    "        new_image = preprocess_input(new_image)\n",
    "        return new_image\n",
    "\n",
    "    def init_densenet():\n",
    "        print('[INFO] Init Densenet...')\n",
    "        inp = Input((256, 256, 3))\n",
    "        print('[INFO] import Densenet...')\n",
    "        backbone = DenseNet121(input_tensor=inp, include_top=False,\n",
    "                               weights='../input/densenet-121-weights/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "        print('[INFO] import Densenet DONE')\n",
    "        x = backbone.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Lambda(lambda x: K.expand_dims(x, axis=-1))(x)\n",
    "        x = AveragePooling1D(4)(x)\n",
    "        out = Lambda(lambda x: x[:, :, 0])(x)\n",
    "        m = Model(inp, out)\n",
    "        print('[INFO] Init Densenet DONE.')\n",
    "        return m\n",
    "\n",
    "    m = init_densenet()\n",
    "\n",
    "    print('[INFO] Start Image Features_Extraction...')\n",
    "    img_size = 256\n",
    "    batch_size = 16\n",
    "    ids = df[id_column].values\n",
    "    n_batches = len(ids) // batch_size + 1\n",
    "    features = {}\n",
    "    for b in tqdm(range(n_batches)):\n",
    "        start = b * batch_size\n",
    "        end = (b + 1) * batch_size\n",
    "        batch_ids = ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_ids), img_size, img_size, 3))\n",
    "        for i, id_ in enumerate(batch_ids):\n",
    "            \n",
    "            #image_name = '{}-{}.jpg'.format(id_, 1)\n",
    "            #image_path = jp(input_dir, subfolder, image_name)\n",
    "            image_path= df.loc[df['id']==id_][column_path].values[0]\n",
    "            try:\n",
    "                batch_images[i] = load_image(256,url=image_path)\n",
    "            except:\n",
    "                continue\n",
    "        batch_preds = m.predict(batch_images)\n",
    "        for i, id_ in enumerate(batch_ids):\n",
    "            features[id_] = batch_preds[i]\n",
    "\n",
    "    df_features = pd.DataFrame.from_dict(features, orient='index')\n",
    "    df_features.rename(columns=lambda k: 'img_{}'.format(k), inplace=True)\n",
    "    df_features.reset_index(inplace=True)\n",
    "    df_features.rename(columns={df_features.columns[0]: id_column}, inplace=True)\n",
    "    n_components = 200\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    X = df_features[['img_{}'.format(k) for k in range(256)]].values\n",
    "    svd.fit(X)\n",
    "    print('fit done')\n",
    "    X_svd = svd.transform(X)\n",
    "    X_svd = pd.DataFrame(X_svd, columns=['img_svd_{}'.format(i) for i in range(n_components)])\n",
    "    X_svd[id_column] = df.id.values.tolist()\n",
    "\n",
    "    df = pd.concat([df.set_index(id_column), X_svd.set_index(id_column)], sort=False, axis=1).reset_index()\n",
    "    df.rename(columns={df.columns[0]: id_column}, inplace=True)\n",
    "    print('[INFO] Image Features_Extraction DONE.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here______________________________\n",
      "[]\n",
      "[INFO] Init Densenet...\n",
      "[INFO] import Densenet...\n",
      "[INFO] import Densenet DONE\n",
      "[INFO] Init Densenet DONE.\n",
      "[INFO] Start Image Features_Extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6908fe2a8bfe47248b8ecb0cfffe4b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=556), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = extract_features(train, 'id', 'image_url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 2169: expected 31 fields, saw 33\\nSkipping line 4823: expected 31 fields, saw 37\\nSkipping line 4860: expected 31 fields, saw 37\\nSkipping line 7343: expected 31 fields, saw 37\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dropping Columns...\n",
      "[INFO] Auto Features Processing\n",
      "Built 403 features\n",
      "EntitySet scattered to 1 workers in 1 seconds\n",
      "Elapsed: 00:01 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 1/1 chunks\n",
      "[INFO] Text Features processing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17fe89ce82846f78283e6bc5e7f0685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "[INFO] Start count vectorize\n",
      "[INFO] Start TFDIDF\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c477971abc844a29d4f0cf3650c6edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7ff2d25c614a4bb69f9338cbd98fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory usage of dataframe is 0.06 MB\n",
      "Memory usage after optimization is: 0.04 MB\n",
      "Decreased by 31.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/utils.py\", line 713, in log_errors\n",
      "    yield\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1223, in _close\n",
      "    quiet_exceptions=(CancelledError,),\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 584, in with_timeout\n",
      "    chain_future(future_converted, result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 166, in chain_future\n",
      "    future_add_done_callback(a, copy)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 262, in future_add_done_callback\n",
      "    callback(future)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 160, in copy\n",
      "    elif a.exception() is not None:\n",
      "concurrent.futures._base.CancelledError\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/utils.py\", line 713, in log_errors\n",
      "    yield\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 992, in _reconnect\n",
      "    yield self._close()\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 729, in run\n",
      "    value = future.result()\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1223, in _close\n",
      "    quiet_exceptions=(CancelledError,),\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 584, in with_timeout\n",
      "    chain_future(future_converted, result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 166, in chain_future\n",
      "    future_add_done_callback(a, copy)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 262, in future_add_done_callback\n",
      "    callback(future)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 160, in copy\n",
      "    elif a.exception() is not None:\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"X_train.csv\", index_col=0, error_bad_lines=False)\n",
    "len_train = len(train)\n",
    "test = pd.read_csv(\"X_test.csv\", index_col=0, error_bad_lines=False)\n",
    "\n",
    "train = train.reset_index()\n",
    "test= test.reset_index()\n",
    "train['id']  = train['id'].astype(str)+'_'+'train'\n",
    "test['id']  = test['id'].astype(str)+'_'+'test'\n",
    "\n",
    "#t\n",
    "y = pd.read_csv(\"y_train.csv\", index_col=0)\n",
    "train_test = pd.concat((train, test), axis=0)\n",
    "train_test = features_engineering(train_test.sample(10))\n",
    "#train_test = train_test.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>images_count</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_size</th>\n",
       "      <th>material</th>\n",
       "      <th>age</th>\n",
       "      <th>warranty</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_sum</th>\n",
       "      <th>tfidf_mean</th>\n",
       "      <th>tfidf_len</th>\n",
       "      <th>category_enfance</th>\n",
       "      <th>category_librairie</th>\n",
       "      <th>category_loisirs</th>\n",
       "      <th>category_mobilier - deco</th>\n",
       "      <th>category_mode</th>\n",
       "      <th>log_price</th>\n",
       "      <th>log_len_product_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>480_test</td>\n",
       "      <td>4</td>\n",
       "      <td>3152.0</td>\n",
       "      <td>2124.0</td>\n",
       "      <td>Retrouvez dans ce livre datant de 1890 tous le...</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.449219</td>\n",
       "      <td>0.188477</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.257812</td>\n",
       "      <td>5.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8634_train</td>\n",
       "      <td>1</td>\n",
       "      <td>616.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;Livre d'occasion écrit par Elisa Vi...</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.732422</td>\n",
       "      <td>0.133179</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.931641</td>\n",
       "      <td>6.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>283_train</td>\n",
       "      <td>5</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>Voiture Miniature Porsche Boxster  gris métall...</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.134766</td>\n",
       "      <td>5.410156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1450_train</td>\n",
       "      <td>5</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3008.0</td>\n",
       "      <td>Ancienne à restaurer , en métal , girouette de...</td>\n",
       "      <td>missing</td>\n",
       "      <td>Métal</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.892578</td>\n",
       "      <td>4.316406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2817_test</td>\n",
       "      <td>1</td>\n",
       "      <td>616.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;Livre d'occasion écrit par Alexandr...</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.732422</td>\n",
       "      <td>0.133179</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.791992</td>\n",
       "      <td>6.691406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>251_test</td>\n",
       "      <td>3</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>Écharpe femme en laine marron et blanche. Dime...</td>\n",
       "      <td>missing</td>\n",
       "      <td>Laine</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.484375</td>\n",
       "      <td>4.277344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>397_train</td>\n",
       "      <td>3</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>Foulard  Gold by Alfredo Versace fabriqué en I...</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.583984</td>\n",
       "      <td>5.050781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2785_test</td>\n",
       "      <td>5</td>\n",
       "      <td>485.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>La tète et le corps soont en maillechort (alli...</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.394531</td>\n",
       "      <td>4.746094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8720_train</td>\n",
       "      <td>3</td>\n",
       "      <td>4608.0</td>\n",
       "      <td>2392.0</td>\n",
       "      <td>6 Verres à vin Lausitzer Weibwasser Design - M...</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.044922</td>\n",
       "      <td>5.367188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8788_train</td>\n",
       "      <td>6</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>Jean bicolore pour homme de la marque KAPORAL....</td>\n",
       "      <td>40</td>\n",
       "      <td>100% coton</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.197266</td>\n",
       "      <td>5.964844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1450_train</td>\n",
       "      <td>5</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3008.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>Métal</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.892578</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>251_test</td>\n",
       "      <td>3</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>Laine</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.484375</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2785_test</td>\n",
       "      <td>5</td>\n",
       "      <td>485.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.394531</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2817_test</td>\n",
       "      <td>1</td>\n",
       "      <td>616.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.732422</td>\n",
       "      <td>0.133179</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.791992</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>283_train</td>\n",
       "      <td>5</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.134766</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>397_train</td>\n",
       "      <td>3</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.583984</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>480_test</td>\n",
       "      <td>4</td>\n",
       "      <td>3152.0</td>\n",
       "      <td>2124.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.449219</td>\n",
       "      <td>0.188477</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.257812</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8634_train</td>\n",
       "      <td>1</td>\n",
       "      <td>616.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.732422</td>\n",
       "      <td>0.133179</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.931641</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8720_train</td>\n",
       "      <td>3</td>\n",
       "      <td>4608.0</td>\n",
       "      <td>2392.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.044922</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8788_train</td>\n",
       "      <td>6</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>100% coton</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.197266</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 525 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  images_count  image_width  image_height  \\\n",
       "0     480_test             4       3152.0        2124.0   \n",
       "1   8634_train             1        616.0         616.0   \n",
       "2    283_train             5       1100.0        1100.0   \n",
       "3   1450_train             5       2000.0        3008.0   \n",
       "4    2817_test             1        616.0         616.0   \n",
       "5     251_test             3       1536.0        1536.0   \n",
       "6    397_train             3       1100.0        1100.0   \n",
       "7    2785_test             5        485.0        1024.0   \n",
       "8   8720_train             3       4608.0        2392.0   \n",
       "9   8788_train             6       2448.0        2448.0   \n",
       "10  1450_train             5       2000.0        3008.0   \n",
       "11    251_test             3       1536.0        1536.0   \n",
       "12   2785_test             5        485.0        1024.0   \n",
       "13   2817_test             1        616.0         616.0   \n",
       "14   283_train             5       1100.0        1100.0   \n",
       "15   397_train             3       1100.0        1100.0   \n",
       "16    480_test             4       3152.0        2124.0   \n",
       "17  8634_train             1        616.0         616.0   \n",
       "18  8720_train             3       4608.0        2392.0   \n",
       "19  8788_train             6       2448.0        2448.0   \n",
       "\n",
       "                                  product_description product_size  \\\n",
       "0   Retrouvez dans ce livre datant de 1890 tous le...      missing   \n",
       "1   <p><strong>Livre d'occasion écrit par Elisa Vi...      missing   \n",
       "2   Voiture Miniature Porsche Boxster  gris métall...      missing   \n",
       "3   Ancienne à restaurer , en métal , girouette de...      missing   \n",
       "4   <p><strong>Livre d'occasion écrit par Alexandr...      missing   \n",
       "5   Écharpe femme en laine marron et blanche. Dime...      missing   \n",
       "6   Foulard  Gold by Alfredo Versace fabriqué en I...      missing   \n",
       "7   La tète et le corps soont en maillechort (alli...      missing   \n",
       "8   6 Verres à vin Lausitzer Weibwasser Design - M...      missing   \n",
       "9   Jean bicolore pour homme de la marque KAPORAL....           40   \n",
       "10                                                NaN      missing   \n",
       "11                                                NaN      missing   \n",
       "12                                                NaN      missing   \n",
       "13                                                NaN      missing   \n",
       "14                                                NaN      missing   \n",
       "15                                                NaN      missing   \n",
       "16                                                NaN      missing   \n",
       "17                                                NaN      missing   \n",
       "18                                                NaN      missing   \n",
       "19                                                NaN           40   \n",
       "\n",
       "       material      age  warranty    year  ... tfidf_sum  tfidf_mean  \\\n",
       "0       missing  missing         1  1890.0  ...  2.449219    0.188477   \n",
       "1       missing  missing         1  2002.0  ...  1.732422    0.133179   \n",
       "2       missing  missing         1     NaN  ...  1.000000    0.076904   \n",
       "3         Métal  missing         1     NaN  ...  1.000000    0.076904   \n",
       "4       missing  missing         1  2014.0  ...  1.732422    0.133179   \n",
       "5         Laine  missing         1     NaN  ...  1.000000    0.076904   \n",
       "6       missing  missing         1     NaN  ...  1.000000    0.076904   \n",
       "7       missing  missing         1     NaN  ...  1.000000    0.076904   \n",
       "8       missing  missing         1     NaN  ...  1.000000    0.076904   \n",
       "9   100% coton   missing         1     NaN  ...  1.000000    0.076904   \n",
       "10        Métal  missing         1     NaN  ...  1.000000    0.076904   \n",
       "11        Laine  missing         1     NaN  ...  1.000000    0.076904   \n",
       "12      missing  missing         1     NaN  ...  1.000000    0.076904   \n",
       "13      missing  missing         1  2014.0  ...  1.732422    0.133179   \n",
       "14      missing  missing         1     NaN  ...  1.000000    0.076904   \n",
       "15      missing  missing         1     NaN  ...  1.000000    0.076904   \n",
       "16      missing  missing         1  1890.0  ...  2.449219    0.188477   \n",
       "17      missing  missing         1  2002.0  ...  1.732422    0.133179   \n",
       "18      missing  missing         1     NaN  ...  1.000000    0.076904   \n",
       "19  100% coton   missing         1     NaN  ...  1.000000    0.076904   \n",
       "\n",
       "    tfidf_len category_enfance  category_librairie  category_loisirs  \\\n",
       "0           6              0.0                 0.0               1.0   \n",
       "1           3              0.0                 1.0               0.0   \n",
       "2           1              1.0                 0.0               0.0   \n",
       "3           1              0.0                 0.0               0.0   \n",
       "4           3              0.0                 1.0               0.0   \n",
       "5           1              0.0                 0.0               0.0   \n",
       "6           1              0.0                 0.0               0.0   \n",
       "7           1              0.0                 0.0               1.0   \n",
       "8           1              0.0                 0.0               0.0   \n",
       "9           1              0.0                 0.0               0.0   \n",
       "10          1              0.0                 0.0               0.0   \n",
       "11          1              0.0                 0.0               0.0   \n",
       "12          1              0.0                 0.0               1.0   \n",
       "13          3              0.0                 1.0               0.0   \n",
       "14          1              1.0                 0.0               0.0   \n",
       "15          1              0.0                 0.0               0.0   \n",
       "16          6              0.0                 0.0               1.0   \n",
       "17          3              0.0                 1.0               0.0   \n",
       "18          1              0.0                 0.0               0.0   \n",
       "19          1              0.0                 0.0               0.0   \n",
       "\n",
       "    category_mobilier - deco category_mode log_price  \\\n",
       "0                        0.0           0.0  3.257812   \n",
       "1                        0.0           0.0  1.931641   \n",
       "2                        0.0           0.0  3.134766   \n",
       "3                        1.0           0.0  3.892578   \n",
       "4                        0.0           0.0  1.791992   \n",
       "5                        0.0           1.0  2.484375   \n",
       "6                        0.0           1.0  3.583984   \n",
       "7                        0.0           0.0  4.394531   \n",
       "8                        1.0           0.0  3.044922   \n",
       "9                        0.0           1.0  2.197266   \n",
       "10                       1.0           0.0  3.892578   \n",
       "11                       0.0           1.0  2.484375   \n",
       "12                       0.0           0.0  4.394531   \n",
       "13                       0.0           0.0  1.791992   \n",
       "14                       0.0           0.0  3.134766   \n",
       "15                       0.0           1.0  3.583984   \n",
       "16                       0.0           0.0  3.257812   \n",
       "17                       0.0           0.0  1.931641   \n",
       "18                       1.0           0.0  3.044922   \n",
       "19                       0.0           1.0  2.197266   \n",
       "\n",
       "   log_len_product_description  \n",
       "0                     5.390625  \n",
       "1                     6.625000  \n",
       "2                     5.410156  \n",
       "3                     4.316406  \n",
       "4                     6.691406  \n",
       "5                     4.277344  \n",
       "6                     5.050781  \n",
       "7                     4.746094  \n",
       "8                     5.367188  \n",
       "9                     5.964844  \n",
       "10                         NaN  \n",
       "11                         NaN  \n",
       "12                         NaN  \n",
       "13                         NaN  \n",
       "14                         NaN  \n",
       "15                         NaN  \n",
       "16                         NaN  \n",
       "17                         NaN  \n",
       "18                         NaN  \n",
       "19                         NaN  \n",
       "\n",
       "[20 rows x 525 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7094f1dbf200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#fp.drop('id', inplace=True, axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fp' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/utils.py\", line 713, in log_errors\n",
      "    yield\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1223, in _close\n",
      "    quiet_exceptions=(CancelledError,),\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 584, in with_timeout\n",
      "    chain_future(future_converted, result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 166, in chain_future\n",
      "    future_add_done_callback(a, copy)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 262, in future_add_done_callback\n",
      "    callback(future)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 160, in copy\n",
      "    elif a.exception() is not None:\n",
      "concurrent.futures._base.CancelledError\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/utils.py\", line 713, in log_errors\n",
      "    yield\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 992, in _reconnect\n",
      "    yield self._close()\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 729, in run\n",
      "    value = future.result()\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/distributed/client.py\", line 1223, in _close\n",
      "    quiet_exceptions=(CancelledError,),\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/gen.py\", line 584, in with_timeout\n",
      "    chain_future(future_converted, result)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 166, in chain_future\n",
      "    future_add_done_callback(a, copy)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 262, in future_add_done_callback\n",
      "    callback(future)\n",
      "  File \"/anaconda3/envs/datascience-env/lib/python3.6/site-packages/tornado/concurrent.py\", line 160, in copy\n",
      "    elif a.exception() is not None:\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "train_test.reset_index(drop=True)\n",
    "#fp.drop('id', inplace=True, axis=1)\n",
    "train = train_test.iloc[:len_train, :]\n",
    "test = train_test.iloc[len_train:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Auto Features Processing\n",
      "Built 507 features\n",
      "EntitySet scattered to 5 workers in 5 seconds\n",
      "Elapsed: 00:35 | Remaining: 00:00 | Progress: 100%|██████████████████████████████████████████| Calculated: 10/10 chunks\n",
      "There are 212 features to remove.\n",
      "The number of features that passed the collinearity threshold:  295\n"
     ]
    }
   ],
   "source": [
    "fp = auto_features(train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'images_count',\n",
       " 'image_width',\n",
       " 'image_height',\n",
       " 'product_size',\n",
       " 'age',\n",
       " 'color',\n",
       " 'product_width',\n",
       " 'condition',\n",
       " 'product_length',\n",
       " 'shoe_size',\n",
       " 'brand',\n",
       " 'product_height',\n",
       " 'NUM_WORDS(image_url)',\n",
       " 'NUM_WORDS(product_description)',\n",
       " 'brand.STD(data.product_width)',\n",
       " 'brand.STD(data.product_length)',\n",
       " 'brand.STD(data.product_height)',\n",
       " 'brand.MAX(data.image_width)',\n",
       " 'brand.MAX(data.image_height)',\n",
       " 'brand.MAX(data.year)',\n",
       " 'brand.MAX(data.shoe_size)',\n",
       " 'brand.MAX(data.price)',\n",
       " 'brand.SKEW(data.product_width)',\n",
       " 'brand.SKEW(data.product_length)',\n",
       " 'brand.SKEW(data.product_height)',\n",
       " 'brand.MIN(data.shoe_size)',\n",
       " 'brand.MIN(data.weight)',\n",
       " 'brand.MIN(data.price)',\n",
       " 'brand.MEAN(data.image_width)',\n",
       " 'brand.MEAN(data.image_height)',\n",
       " 'brand.MEAN(data.year)',\n",
       " 'brand.MEAN(data.shoe_size)',\n",
       " 'brand.COUNT(data)',\n",
       " 'brand.NUM_UNIQUE(data.store_name)',\n",
       " 'brand.MODE(data.product_size)',\n",
       " 'brand.MODE(data.material)',\n",
       " 'brand.MODE(data.age)',\n",
       " 'brand.MODE(data.warranty)',\n",
       " 'brand.MODE(data.color)',\n",
       " 'brand.MODE(data.condition)',\n",
       " 'brand.MODE(data.author)',\n",
       " 'brand.MODE(data.editor)',\n",
       " 'brand.MODE(data.category)',\n",
       " 'brand.MODE(data.sub_category_1)',\n",
       " 'brand.MODE(data.sub_category_2)',\n",
       " 'brand.MODE(data.sub_category_3)',\n",
       " 'brand.MODE(data.sub_category_4)',\n",
       " 'brand.MODE(data.product_name)',\n",
       " 'brand.MODE(data.store_name)',\n",
       " 'category.SUM(data.images_count)',\n",
       " 'category.SUM(data.price)',\n",
       " 'category.STD(data.images_count)',\n",
       " 'category.STD(data.image_width)',\n",
       " 'category.STD(data.image_height)',\n",
       " 'category.STD(data.shoe_size)',\n",
       " 'category.STD(data.weight)',\n",
       " 'category.MAX(data.images_count)',\n",
       " 'category.MAX(data.image_width)',\n",
       " 'category.MAX(data.image_height)',\n",
       " 'category.MAX(data.product_width)',\n",
       " 'category.MAX(data.shoe_size)',\n",
       " 'category.MAX(data.price)',\n",
       " 'category.SKEW(data.images_count)',\n",
       " 'category.SKEW(data.image_height)',\n",
       " 'category.MIN(data.price)',\n",
       " 'category.NUM_UNIQUE(data.condition)',\n",
       " 'category.NUM_UNIQUE(data.vintage)',\n",
       " 'category.NUM_UNIQUE(data.sub_category_1)',\n",
       " 'category.MODE(data.product_size)',\n",
       " 'category.MODE(data.material)',\n",
       " 'category.MODE(data.age)',\n",
       " 'category.MODE(data.warranty)',\n",
       " 'category.MODE(data.color)',\n",
       " 'category.MODE(data.condition)',\n",
       " 'category.MODE(data.brand)',\n",
       " 'category.MODE(data.author)',\n",
       " 'category.MODE(data.editor)',\n",
       " 'category.MODE(data.sub_category_1)',\n",
       " 'category.MODE(data.sub_category_2)',\n",
       " 'category.MODE(data.sub_category_3)',\n",
       " 'category.MODE(data.sub_category_4)',\n",
       " 'category.MODE(data.product_name)',\n",
       " 'category.MODE(data.store_name)',\n",
       " 'store_name.SUM(data.images_count)',\n",
       " 'store_name.SUM(data.image_width)',\n",
       " 'store_name.SUM(data.weight)',\n",
       " 'store_name.SUM(data.price)',\n",
       " 'store_name.STD(data.product_height)',\n",
       " 'store_name.MAX(data.images_count)',\n",
       " 'store_name.MAX(data.image_width)',\n",
       " 'store_name.MAX(data.image_height)',\n",
       " 'store_name.MAX(data.product_length)',\n",
       " 'store_name.MAX(data.shoe_size)',\n",
       " 'store_name.MAX(data.product_height)',\n",
       " 'store_name.MAX(data.weight)',\n",
       " 'store_name.MAX(data.price)',\n",
       " 'store_name.SKEW(data.year)',\n",
       " 'store_name.MIN(data.image_width)',\n",
       " 'store_name.MIN(data.product_width)',\n",
       " 'store_name.MIN(data.product_length)',\n",
       " 'store_name.MIN(data.weight)',\n",
       " 'store_name.MIN(data.price)',\n",
       " 'store_name.MEAN(data.images_count)',\n",
       " 'store_name.MEAN(data.image_width)',\n",
       " 'store_name.MEAN(data.image_height)',\n",
       " 'store_name.MEAN(data.product_width)',\n",
       " 'store_name.MEAN(data.product_length)',\n",
       " 'store_name.MEAN(data.shoe_size)',\n",
       " 'store_name.MEAN(data.weight)',\n",
       " 'store_name.MEAN(data.price)',\n",
       " 'store_name.COUNT(data)',\n",
       " 'store_name.NUM_UNIQUE(data.condition)',\n",
       " 'store_name.NUM_UNIQUE(data.category)',\n",
       " 'store_name.MODE(data.product_size)',\n",
       " 'store_name.MODE(data.material)',\n",
       " 'store_name.MODE(data.age)',\n",
       " 'store_name.MODE(data.warranty)',\n",
       " 'store_name.MODE(data.color)',\n",
       " 'store_name.MODE(data.condition)',\n",
       " 'store_name.MODE(data.brand)',\n",
       " 'store_name.MODE(data.author)',\n",
       " 'store_name.MODE(data.editor)',\n",
       " 'store_name.MODE(data.category)',\n",
       " 'store_name.MODE(data.sub_category_1)',\n",
       " 'store_name.MODE(data.sub_category_2)',\n",
       " 'store_name.MODE(data.sub_category_3)',\n",
       " 'store_name.MODE(data.sub_category_4)',\n",
       " 'store_name.MODE(data.product_name)',\n",
       " 'product_name.STD(data.shoe_size)',\n",
       " 'product_name.SKEW(data.product_width)',\n",
       " 'product_name.SKEW(data.product_length)',\n",
       " 'product_name.SKEW(data.shoe_size)',\n",
       " 'product_name.SKEW(data.product_height)',\n",
       " 'product_name.MODE(data.product_size)',\n",
       " 'product_name.MODE(data.material)',\n",
       " 'product_name.MODE(data.age)',\n",
       " 'product_name.MODE(data.warranty)',\n",
       " 'product_name.MODE(data.color)',\n",
       " 'product_name.MODE(data.condition)',\n",
       " 'product_name.MODE(data.brand)',\n",
       " 'product_name.MODE(data.author)',\n",
       " 'product_name.MODE(data.editor)',\n",
       " 'product_name.MODE(data.category)',\n",
       " 'product_name.MODE(data.sub_category_1)',\n",
       " 'product_name.MODE(data.sub_category_2)',\n",
       " 'product_name.MODE(data.sub_category_3)',\n",
       " 'product_name.MODE(data.sub_category_4)',\n",
       " 'product_name.MODE(data.store_name)',\n",
       " 'material.STD(data.year)',\n",
       " 'material.MAX(data.image_width)',\n",
       " 'material.MAX(data.image_height)',\n",
       " 'material.MAX(data.price)',\n",
       " 'material.SKEW(data.year)',\n",
       " 'material.MIN(data.image_width)',\n",
       " 'material.MIN(data.image_height)',\n",
       " 'material.MIN(data.shoe_size)',\n",
       " 'material.MIN(data.weight)',\n",
       " 'material.MIN(data.price)',\n",
       " 'material.MEAN(data.image_width)',\n",
       " 'material.MEAN(data.price)',\n",
       " 'material.NUM_UNIQUE(data.store_name)',\n",
       " 'material.MODE(data.product_size)',\n",
       " 'material.MODE(data.age)',\n",
       " 'material.MODE(data.warranty)',\n",
       " 'material.MODE(data.color)',\n",
       " 'material.MODE(data.condition)',\n",
       " 'material.MODE(data.brand)',\n",
       " 'material.MODE(data.author)',\n",
       " 'material.MODE(data.editor)',\n",
       " 'material.MODE(data.category)',\n",
       " 'material.MODE(data.sub_category_1)',\n",
       " 'material.MODE(data.sub_category_2)',\n",
       " 'material.MODE(data.sub_category_3)',\n",
       " 'material.MODE(data.sub_category_4)',\n",
       " 'material.MODE(data.product_name)',\n",
       " 'material.MODE(data.store_name)',\n",
       " 'product_description_svd_7',\n",
       " 'product_description_svd_9',\n",
       " 'product_description_svd_10',\n",
       " 'product_description_svd_11']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_test.iloc[:len_train, :]\n",
    "test = train_test.iloc[len_train:, :]\n",
    "test_id = test.index\n",
    "train['label'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8880, 75)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['images_count', 'image_width', 'image_height', 'year', 'product_width',\n",
       "       'product_length', 'shoe_size', 'product_height', 'price', 'image_width',\n",
       "       'image_height', 'product_width', 'product_length', 'shoe_size',\n",
       "       'product_height', 'price', 'NUM_WORDS(product_description)',\n",
       "       'brand.MAX(data.image_width)', 'brand.MAX(data.image_height)',\n",
       "       'brand.MAX(data.year)', 'brand.MAX(data.shoe_size)',\n",
       "       'brand.MAX(data.price)', 'brand.MIN(data.shoe_size)',\n",
       "       'brand.MIN(data.price)', 'brand.MEAN(data.image_width)',\n",
       "       'brand.MEAN(data.image_height)', 'brand.MEAN(data.year)',\n",
       "       'brand.COUNT(data)', 'brand.NUM_UNIQUE(data.store_name)',\n",
       "       'category.SUM(data.images_count)', 'category.SUM(data.price)',\n",
       "       'category.STD(data.image_width)', 'category.STD(data.image_height)',\n",
       "       'category.MAX(data.images_count)', 'category.MAX(data.image_width)',\n",
       "       'category.MAX(data.image_height)', 'category.MAX(data.product_width)',\n",
       "       'category.MAX(data.shoe_size)', 'category.MAX(data.price)',\n",
       "       'category.NUM_UNIQUE(data.sub_category_1)',\n",
       "       'store_name.SUM(data.images_count)', 'store_name.SUM(data.price)',\n",
       "       'store_name.STD(data.product_height)',\n",
       "       'store_name.MAX(data.images_count)', 'store_name.MAX(data.image_width)',\n",
       "       'store_name.MAX(data.image_height)',\n",
       "       'store_name.MAX(data.product_length)', 'store_name.MAX(data.shoe_size)',\n",
       "       'store_name.MAX(data.product_height)', 'store_name.MAX(data.price)',\n",
       "       'store_name.SKEW(data.year)', 'store_name.MIN(data.image_width)',\n",
       "       'store_name.MIN(data.product_width)',\n",
       "       'store_name.MIN(data.product_length)', 'store_name.MIN(data.weight)',\n",
       "       'store_name.MIN(data.price)', 'store_name.MEAN(data.images_count)',\n",
       "       'store_name.MEAN(data.image_width)',\n",
       "       'store_name.MEAN(data.image_height)',\n",
       "       'store_name.MEAN(data.product_width)',\n",
       "       'store_name.MEAN(data.product_length)',\n",
       "       'store_name.MEAN(data.shoe_size)', 'store_name.MEAN(data.price)',\n",
       "       'store_name.COUNT(data)', 'store_name.NUM_UNIQUE(data.condition)',\n",
       "       'store_name.NUM_UNIQUE(data.category)',\n",
       "       'material.MAX(data.image_height)', 'material.MAX(data.price)',\n",
       "       'material.MIN(data.image_width)', 'material.MIN(data.image_height)',\n",
       "       'material.MIN(data.shoe_size)', 'material.MIN(data.price)',\n",
       "       'material.MEAN(data.image_width)', 'material.MEAN(data.price)',\n",
       "       'material.NUM_UNIQUE(data.store_name)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "train_X = train.copy()\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train_X.select_dtypes([np.number]).fillna(-1), train_y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(train_X.select_dtypes([np.number]).fillna(-1))\n",
    "X_selected_df = pd.DataFrame(X_new, columns=[train_X.select_dtypes([np.number]).fillna(-1).columns[i] for i in range(len(train_X.select_dtypes([np.number]).fillna(-1).columns)) if model.get_support()[i]])\n",
    "print(X_selected_df.shape)\n",
    "X_selected_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11850 entries, 0 to 11849\n",
      "Columns: 181 entries, id to product_description_svd_11\n",
      "dtypes: category(80), float16(82), float32(8), float64(11)\n",
      "memory usage: 5.5 MB\n"
     ]
    }
   ],
   "source": [
    "train_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_target_and_df(train,label_column):\n",
    "    return train.drop([label_column], axis=1),train[label_column]\n",
    "    \n",
    "def run_randomforest_classifier(train, test, label_column,scoring='accuracy'):\n",
    "    \n",
    "    train,target = split_target_and_df(train,label_column)\n",
    "    \n",
    "    params = {'bootstrap': True, \n",
    "              'class_weight': None, \n",
    "              'criterion': 'gini', \n",
    "              'max_depth': None,\n",
    "              'max_features': 'auto', \n",
    "              'max_leaf_nodes': None, \n",
    "              'min_impurity_decrease': 0.0, \n",
    "              'min_impurity_split': None,\n",
    "              'min_samples_leaf': 1,\n",
    "              'min_samples_split': 2, \n",
    "              'min_weight_fraction_leaf': 0.0, \n",
    "              'n_estimators': 10,\n",
    "              'n_jobs': -1, \n",
    "              'oob_score': False, \n",
    "              'random_state': None, \n",
    "              'verbose': 0, \n",
    "              'warm_start': False}\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(train, target)\n",
    "    pred_train = model.predict(train)\n",
    "    pred_test = model.predict(test)\n",
    "    \n",
    "    cv_scores = cross_val_score(model, train, target, cv=5, scoring=scoring)\n",
    "    print(cv_scores)\n",
    "    print('RF CV mean : %.2f ' % (np.mean(cv_scores)))\n",
    "    print('RF CV std : %.2f ' % (np.std(cv_scores)))\n",
    "        \n",
    "    print(\"True Distribution:\")\n",
    "    print(pd.value_counts(target, normalize=True).sort_index())\n",
    "    print(\"Train Predicted Distribution:\")\n",
    "    print(pd.value_counts(pred_train, normalize=True).sort_index())\n",
    "    print(\"Test Predicted Distribution:\")\n",
    "    print(pd.value_counts(pred_test, normalize=True).sort_index())\n",
    "    \n",
    "    features_importances = pd.Series(model.feature_importances_, index=train.columns)\n",
    "    features_importances.nlargest(25).plot(kind='barh')\n",
    "    \n",
    "    return pred_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'https://d1kvfoyrif6wzg.cloudfront.net/assets/images/None/main/100_6771_3b0f897.JPG'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-5c79a72b956d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_randomforest_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-8e905f0b109d>\u001b[0m in \u001b[0;36mrun_randomforest_classifier\u001b[1;34m(train, test, label_column, scoring)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mpred_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mpred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'https://d1kvfoyrif6wzg.cloudfront.net/assets/images/None/main/100_6771_3b0f897.JPG'"
     ]
    }
   ],
   "source": [
    "pred_test = run_randomforest_classifier(train,test,\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TODO : \"LightGBM validation CV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 2\n",
    "pred_test = run_lgbm(train, test,'label',test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(train, test, label_column):\n",
    "    target = train[label_column]\n",
    "    train = train.drop([label_column], axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train)\n",
    "    test_scaled = scaler.fit_transform(test)\n",
    "    \n",
    "    svm_params = {'C': 1.0, \n",
    "                  'cache_size': 200, \n",
    "                  'class_weight': None, \n",
    "                  'coef0': 0.0, \n",
    "                  'decision_function_shape': 'ovr', \n",
    "                  'degree': 3, 'gamma': \n",
    "                  'auto_deprecated', \n",
    "                  'kernel': 'rbf', \n",
    "                  'max_iter': -1, \n",
    "                  'probability': False, \n",
    "                  'random_state': None, \n",
    "                  'shrinking': True, \n",
    "                  'tol': 0.001, \n",
    "                  'verbose': False}\n",
    "    \n",
    "    svc=SVC() \n",
    "    svc.fit(train_scaled,target)\n",
    "    y_pred_train=svc.predict(train_scaled)\n",
    "    score = accuracy_score(target,y_pred_train)\n",
    "    print('Accuracy Score: %.2f' % (score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_svm(train, test, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_voting_classifier(train, test, label_column):\n",
    "    \n",
    "    target = train[label_column]\n",
    "    train = train.drop([label_column], axis=1)\n",
    "    \n",
    "    ab_params = {'algorithm': 'SAMME.R', \n",
    "                 'base_estimator': None, \n",
    "                 'learning_rate': 0.1, \n",
    "                 'n_estimators': 20, \n",
    "                 'random_state': None}\n",
    "    \n",
    "    gbc_params = {'criterion': 'friedman_mse', \n",
    "                  'init': None, 'learning_rate': 0.1, \n",
    "                  'loss': 'deviance', \n",
    "                  'max_depth': 30, \n",
    "                  'max_features': None, \n",
    "                  'max_leaf_nodes': None, \n",
    "                  'min_impurity_decrease': 0.0, \n",
    "                  'min_impurity_split': None, \n",
    "                  'min_samples_leaf': 1, \n",
    "                  'min_samples_split': 2, \n",
    "                  'min_weight_fraction_leaf': 0.0, \n",
    "                  'n_estimators': 100, \n",
    "                  'n_iter_no_change': None, \n",
    "                  'presort': 'auto', \n",
    "                  'random_state': None, \n",
    "                  'subsample': 1.0, \n",
    "                  'tol': 0.0001, \n",
    "                  'validation_fraction': 0.1, \n",
    "                  'verbose': 0, \n",
    "                  'warm_start': False}\n",
    "    \n",
    "    bc_params = {'base_estimator': None, \n",
    "                 'bootstrap': True, \n",
    "                 'bootstrap_features': False, \n",
    "                 'max_features': 10, \n",
    "                 'max_samples': 1.0, \n",
    "                 'n_estimators': 20, \n",
    "                 'n_jobs': None, \n",
    "                 'oob_score': False, \n",
    "                 'random_state': None, \n",
    "                 'verbose': 0, \n",
    "                 'warm_start': False}\n",
    "    \n",
    "    clf1 = AdaBoostClassifier(**ab_params)\n",
    "    clf2 = GradientBoostingClassifier(**gbc_params)\n",
    "    clf3 = BaggingClassifier(**bc_params)\n",
    "    vote_clf = VotingClassifier(estimators=[('ab', clf1), ('gbc', clf2), ('bc', clf3)], weights=[0.2,1.7,0.6], voting='soft')\n",
    "    vote_clf = vote_clf.fit(train, target)\n",
    "    \n",
    "    pred_train = vote_clf.predict_proba(train)\n",
    "    pred_cv = cross_val_predict(vote_clf, train, np.ravel(target),\n",
    "                            method='predict_proba', cv=5, n_jobs=-1)\n",
    "    pred_test = vote_clf.predict_proba(test)\n",
    "    \n",
    "    print(\"LogLoss on train sample \", log_loss(y_pred=pred_train, y_true=target))\n",
    "    print(\"LogLoss on train sample (CV): \", log_loss(y_pred=pred_cv, y_true=target))\n",
    "    \n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = run_voting_classifier(train, test, \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb_classifier(train, test, label_column):\n",
    "    \n",
    "    target = train[label_column]\n",
    "    train = train.drop([label_column], axis=1)\n",
    "\n",
    "    params = {'objective' : 'multi:softprob', \n",
    "              'num_class'  : 3,\n",
    "              'eval_metric' : 'mlogloss',\n",
    "              'nthread' : -1, \n",
    "              'booster' : \"gbtree\",\n",
    "              'gamma' : 0.1, \n",
    "              'max_depth' : 5,\n",
    "              'eta' : 0.1,\n",
    "              'min_child_weight'  : 0.7\n",
    "             }\n",
    "\n",
    "    clf_xgb = XGBClassifier(**params)\n",
    "\n",
    "    ppl = Pipeline([(\"clf\", clf_xgb)])\n",
    "\n",
    "    ppl.fit(train, np.ravel(y))\n",
    "\n",
    "    pred_train = ppl.predict_proba(train)\n",
    "    pred_cv = cross_val_predict(ppl, train, np.ravel(y),\n",
    "                                method='predict_proba', cv=5, n_jobs=-1,verbose=1)\n",
    "\n",
    "    print(\"LogLoss on train sample:\",log_loss(y_pred=pred_train, y_true=y))\n",
    "    print(\"LogLoss on train sample (CV):\",log_loss(y_pred=pred_cv, y_true=y))\n",
    "    \n",
    "    pred_test = ppl.predict_proba(test)\n",
    "    return pred_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss on train sample: 0.24616178027238814\n",
      "LogLoss on train sample (CV): 1.4492864786074622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.06511849, 0.8484266 , 0.08645495],\n",
       "       [0.20762211, 0.47952724, 0.31285062],\n",
       "       [0.06228445, 0.33334592, 0.60436964],\n",
       "       ...,\n",
       "       [0.07604016, 0.13513353, 0.78882635],\n",
       "       [0.05678065, 0.37313035, 0.57008904],\n",
       "       [0.21156481, 0.36582002, 0.42261523]], dtype=float32)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_xgb_classifier(train._get_numeric_data().fillna(train._get_numeric_data().mean()),test._get_numeric_data().fillna(test._get_numeric_data().mean()),'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-c978ccc675ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mcv_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_log_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RF CV mean : %.2f '\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 573\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "label_column = 'label'\n",
    "train_,target = split_target_and_df(train._get_numeric_data().fillna(train._get_numeric_data().mean()),label_column)\n",
    "\n",
    "model = ExtraTreesClassifier(bootstrap=True , \n",
    "                                         criterion=\"gini\", \n",
    "                                         min_samples_leaf=10, \n",
    "                                         min_samples_split=100, \n",
    "                                         n_estimators=300,\n",
    "                                         random_state = 50,\n",
    "                                         n_jobs = -1)\n",
    "\n",
    "\n",
    "cv_scores = cross_val_score(model, train_ , target, cv=5, scoring='neg_log_loss')\n",
    "print(cv_scores)\n",
    "print('RF CV mean : %.2f ' % (np.mean(cv_scores)))\n",
    "print('RF CV std : %.2f ' % (np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(pred_test, index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(\"submission.csv\", index_label=\"id\", header=['0', '1', '2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
